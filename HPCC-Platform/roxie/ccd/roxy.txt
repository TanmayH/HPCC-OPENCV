To do
=====

Urgent
------
-1 Option to remove unreferenced files from dataDirectory
0 Precheck disk space when copying
3 Try caching only nodes / separate caches for nodes/leaves
7 No incoming dataset provided -> SEH
9 Stats by hour
11 Work out when to terminate on SEHs 
16 more stats by key.
17 SNMP traps on remote exceptions

- hook in Ole's bandwidth limited file copies

- Try to eliminate dynamic allocation from activities and start pooling
- Implement all hthor activities

- farmer pooling can work at the graph level
- ccd pooling of activities can work at the activity level
- use a thread pool for the puller threads


==========================================================================================
Thoughts on inmemory diskread keys.

For queries of the form
memdiskread(fixed-size-records, a IN set, b IN set, c IN set, loop-invariant-expression or d = dval, other fuzzy conditions)

we can transform the d case into d IN set1, where set1 = loop-invariant-expression ? ALL : [dval];

then we can create key segment monitors from all the sets (where an ALL is a wild segment).
(NOTE: assumes fields in question are all memcmp-collatable, or key segment monitors are cleverer).

then we take the key segment monitors and match them up against the available keys (= array of pointers)
then we use the matched up ones to binchop/keyscan, and any unmatched ones (which were not wild) and the fuzzies to postfilter.

We could declare indexes on the preload statement. Or specify number of indexes to keep and let the system create them
as needed with some heuristics to decide when to discard a key and create a replacement.

The key segment monitors give us enough info to create an index (offset of field/length, and collation method if decide to be sophisticated).

You could do a similar thing for indexread, where the set of indexes to pick from is supplied rather than deduced/recreated.

Matrix vehicles at 4 million 48-byte records/node is currently 200 M/node
Allowing another 200M / node for keys would give me 12 keys to play with.
Actually records don't need to be fixed size, but segmonitors would only be applied to fixed-offset fields

To match up available segmonitors with available keys:
1. Sort segmonitors by record offset 
2. For each key, count total size of segmonitors in key that are present in available set.
3. Use the key with highest count. Or scan file, if none.

Keep track of number of requests for each segmonitor. When idle (or whenever) sort the list of requested segmonitors by number 
of requests, and build a new keylist. 

if the following fields show up in requests:
A 100 times
b 10 times
c 2 times
d 0 times

then we build key a.b.c.d first, then b.c.d, then c.a.d

weight by size of field/fact that key is already there if you want to be really clever.

To lookup an inmem using a key segment list, binchop - very similar to index files.

Features needed:
1. using Segmonitors to generate a sorted pointer list.
2. find best key from a segmonitor list.
3. codegen to create segmonitors 
4. binchop using segmonitors and pointer array - WIP
5. CSetKeySegmentMonitor needs to binchop - done
6. key list generation from segmonitor frequency chart.
7. More intelligent segmonitors that can handle little-endian and signed 
    - I was not sure if these were needed, but they are, or you can't do ranges in the SetKeySegmentMonitors.
    - done (could optimize further)

Even without building keys, codegen creating segmonitors for suitable parts of the filter, and using them to do the match 
would pull the "if valuesuppliedforfield" tests out of the inner loop. Not sure if there would be a net gain though. 
Might be pretty close...


Key picking strategies
----------------------

Assume that the relative resolving power of fields is known (maybe determined by order user specifies in KEYED clause)....
Assume that the input set does not affect the resolving power of the key enough to fret about yet.

Sort the available list by resolving power (may already be...)
for each available, if any key matches, discard keys that don't
repeat until keys remaining <= 1.
if there is a tie, use key with fewest trailing components (as we can compress down duplicates and cut binchop).

suffers from local minima

Or, given that the number of indexes is small, we just whizz through them all counting the total size of matching fields.
Tie breaker based on trailing key length...

We could represent a key as a stringset, representing the bytes that are covered in the record.
union of actual key with ideal key gives us overlap.

But that ignores the fact that the order is significant on the actual key.

for each key
  for each field in key
    if (supplied)
      add
    else
      break;

i.e. it's the resolve process, followed by a score. We can undo the resolve very easily (especially if we don't remove them).

if the score is basically the number of keyed bytes, we can reject any key that is shorter than that.
if we sort keys by leading component, we can skip a block if leading component isses (cache the result of the last lookup)
key selection should really be done on farmer rather than repeated on every slave. But key availability will vary from slave to slave
(at least for some timing windows)...

Be sure to early out on a perfect match.

note the field frequency and the combination frequency (not sure how we use them yet)

keep a list of up to N keys that would have been perfect matches. Score them 10 for a use and -1 for a non-use.

From a list of queries with frequency
    - generate a list of fields by frequency (weighted by size, perhaps)
    - score each query by sum of field scores
    - generate key for the query with the biggest score, fields sorted by field score.
    - reset field scores to zero for the fields in that key
    - recalc key scores (removing ones that now score zero)
    - repeat until no more keys permitted or no mor queries.



Rebuild timing
    - Should regenerate every hour or so (start with shorter interval and ramp up)
    - Should regen the same keys on all slaves (No guarantee presently)
    - Should regen at the same time on all slaves

query tracking
    - Age queries by decrementing all by 10 when it fills up, and discarding ones that go -ve.
    - Could do something at rebuild time too?


==========================================================================================

Keyed join record flow
----------------------

Thread 1:
  serialize any context info required by CCD index readers, and pass it at head of each CCD indexread query
  fetch incoming record from previous activity
  use top level key to determine which CCD to send to
  extractIndexReadFields() copies the fields required by the CCD to do the index lookup/filter
  [queryIndexReadInputRecordSize() returns the IRecordSize interface describing these records]
  those fields (plus a pointer to the original row - used later) buffered for passing to the ccd.

Remote CCD indexread activity:
  deserialize context information
  for each row in query:
    createSegmentMonitors(row) to prepare to look it up
    look it up in index to give key record
    for each match
      if (!indexReadFilter(key record, inputrow)
        return fpos (and the supplied original pointer)

Thread 2:
  serialize any context info required by CCD fetchers, and pass it at head of each CCD fetch query
  receive fpos/origptr pairs from all CCD indexread agents
  for each row
    Use file size map to work out which CCD fetch agent to send to
    extractFetchFields(orig_row) copies the fields required by the CCD to do the filter
    that info plus the fpos and origptr are buffered up to pass to the appropriate CCD fetch activity
    
Remote CCD fetch activity
  deserialize context information
  for each row in query:
    fetch disk row at specified offset
    if (!fetchFilter(disk_record, inputrow)
      return extractJoinFields(disk_record, fpos) and the supplied original pointer

Thread 3:
  receive origptr and disk fields from all CCD fetch agents
  for each row
    remove ptr from list of unmatched rows (if we are doing left-only/left outer)
    if (!left_only)
      pass transform(disk_fields, orig_row) to next activity

  Once all done:
    if leftonly or leftouter
      for each unmatched row
        pass transform(dummy, orig_row) to next activity


Note to Gavin:
  For each extract function, I also need to be able to determine (statically) the size of the row
  If any of these support variable size records, I will also need to be told the real size. 
  I wrote the above using two serialize context functions but it would not be that big a deal if they were combined


//=============================================================================

More notes on in memory keys.

If I were to compress the list of matches, and build one key per field, could use key merging to key multiple fields on the fly.
Could be merging quite a number of streams.


1. Good for case where a small set of values specified. Not so good for <value >value type with high card fields.
Good where all records meeting the criteria are going to be processed. Not so good where a choosen() is going to trim it 


AND case - Merging N sorted streams of fposes returning only those that occur in all.
But we also need the OR case for x IN a,b,c

--------------------------------------------------------------------------------

Query/file management

1. Multiple state files rather than monolithic - read them all
2. Is order significant? It is currently....
3. Read state files from central location and/or dali 
   - but cache them locally 
   - use local copy on a restart, central only on manual start or when told to load new
   - commands to load new and to remove old
   - one xml file per managed object
     - wu
     - fileset
     - alias table
     etc
   - allow each file to refer to others
   - UI paradigm to match
   - UI will prevent removal of something referenced, hand edits will not though will pick up problem at runtime
   - xml file per logical file saying where it is? crcs? dates? or not bother? missing information will be ignored/deduced
   - updates while running.
     - slaves need to be able to deliver appropriate data as soon as farmer does
     - i.e. all slaves ready before any farmer ready - but that's not enough since also need to continue to deliver old until all farmers have removed
     - can I get slaves to not need any config?
       - farmer sends info to slaves when they need it
       - so if rid is unknown, slave will send a packet to farmer that will cause it to send a packet indicating what the dll, factory, filename resolution etc is.
       - a delete of a given wu/fileset combination can send drop-rid information to each slave.
       - use a crc as well as/instead of a sequence number?
       - would be a step towards having a single slave farm service multiple farmer sets
       - would be a step towards unifying roxie/eclagent and even roxie/hole
       - child queries out at slave may make this a lot harder
     - at the same time can farmer not key part 1 - if it doesn't have it, ask a slave?
       - what about 1-part indexes?
       - what about FETCH / full keyed join?
     - query configuration settings are at the alias level
     - alias concept replaced by externalname -> (wu + datamappings + settings) concept (wu can also have some info)

     - instead of a roxieId we send a 16-byte guid that farmer can translate on demand to a wu + named datamapping + graph info + whatever structure

     - packet from farmer includes a crc sanity check? Slave will give appropriate error if it does not match.
      - is this enough to fix it?
     - update done via a broadcast with reference to central location? Or do we multicast the contents too?

    - file sets need to be able to have x->y type mappings in.... or use scopes more in thor and roxie 


 - should I still use UDP? Does that mess up the concept?
